# -*- coding: utf-8 -*-
"""Dissertation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tpCgkgRFjAVIoCHKSopfjGj8fpxrDDu8
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import sklearn
import matplotlib.pyplot as plt
# %matplotlib inline
import missingno as msno
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
import keras.optimizers as opti
from keras.layers import Dense, Activation,Dropout
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import mean_absolute_error as mae
from keras.models import Sequential
import keras.optimizers as opti
from keras.layers import Dense, Activation,Dropout

#Access data from CSV file
data = pd.read_csv("/content/drive/MyDrive/forestfires.csv")
data

#Display dataframe
data.shape

#Display column names
data.columns

#Display Number of columns
print(len(data.select_dtypes(include=['object']).columns))
print(len(data.select_dtypes(include=['int64','float64']).columns))
print(len(data.select_dtypes(include=['bool']).columns))

#Display datatypes of different columns
data.dtypes

#Display column info
data.info()

#Check null values
data.isnull().sum()

#Check duplicate
data.duplicated()

#Check unique values
data.nunique()

#Describe dataframe
data.describe()

#Describe columns
data.describe(include=['object'])

#Check missing values
data.isna().sum()

#Display missing no in matrix
msno.matrix(data)

#Display missing values in bar plot
msno.bar(data)

#Display histogram of all the columns
data.hist(figsize=(12,12))

#Visualize missing values
sns.heatmap(data.isna())

#Log of column area
data['Log-area']=np.log10(data['area']+1)

#Display scatter plot
for i in data.describe().columns[:-2]:
    data.plot.scatter(i,'Log-area',grid=True)

#Boxplot showing categorical column "day" affecting outcome
data.boxplot(column='Log-area',by='day')

#Boxplot showing categorical column "month" affecting outcome
data.boxplot(column='Log-area',by='month')

#Encode
data['month']

#Convet data into numerical columns
dummy_set = pd.get_dummies(data.month)
dummy_set

data['day']

dummy_set1 = pd.get_dummies(data.day)
dummy_set1

#Merge dummy dataset into dataframe
merged_data = pd.concat([data, dummy_set, dummy_set1 ], axis=1)
merged_data

pd.get_dummies(data, columns=['month','day'], drop_first=True)

#Label Encoder Month
enc = LabelEncoder()
enc.fit(data['month'])

enc.classes_

#Transform
data['month_encoded']=enc.transform(data['month'])
data.head()

#Label Encoder Day
enc.fit(data['day'])

enc.classes_

#Transform
data['day_encoded']=enc.transform(data['day'])
data.head(10)

print(data.columns)

#Split
test_size=0.4

X = data.drop(['area','Log-area','month','day'], axis=1)
X.head()

y = data['Log-area']
y.head()

#Train and test data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,
                                                    random_state= 156)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
X_train

#Train Model
def rec(m,n,tol):
    if type(m)!='numpy.ndarray':
        m=np.array(m)
    if type(n)!='numpy.ndarray':
        n=np.array(n)
    l=m.size
    percent = 0
    for i in range(l):
        if np.abs(10**m[i]-10**n[i])<=tol:
            percent+=1
    return 100*(percent/l)

tol_max=20

#Grid Search
param_grid = {'C': [0.01,0.1,1, 10], 'epsilon': [10,1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

param_grid = {'max_depth': [5,10,15,20,50], 'max_leaf_nodes': [2,5,10], 'min_samples_leaf': [2,5,10],
             'min_samples_split':[2,5,10]}
grid_RF = GridSearchCV(RandomForestRegressor(),param_grid,refit=True,verbose=0,cv=5)
grid_RF.fit(X_train,y_train)

#Display best parameter
print("Best parameters obtained by Grid Search:",grid_RF.best_params_)

a=grid_RF.predict(X_test)
rmse_rf=np.sqrt(np.mean((y_test-a)**2))
print("RMSE for Random Forest:",rmse_rf)

#Scatter plot to show burned area and error
plt.xlabel("Actual area burned")
plt.ylabel("Error")
plt.grid(True)
plt.scatter(10**(y_test),10**(a)-10**(y_test))

plt.title("Histogram of prediction errors\n",fontsize=18)
plt.xlabel("Prediction error ($ha$)",fontsize=14)
plt.grid(True)
plt.hist(10**(a.reshape(a.size,))-10**(y_test),bins=50)

rec_RF=[]
for i in range(tol_max):
    rec_RF.append(rec(a,y_test,i))

plt.figure(figsize=(5,5))
plt.title("REC curve for the Random Forest\n",fontsize=15)
plt.xlabel("Absolute error (tolerance) in prediction ($ha$)")
plt.ylabel("Percentage of correct prediction")
plt.xticks([i for i in range(0,tol_max+1,5)])
plt.ylim(-10,100)
plt.yticks([i*20 for i in range(6)])
plt.grid(True)
plt.plot(range(tol_max),rec_RF)

#Sequential model
model = Sequential()
model.add(Dense(100, input_dim=12))
model.add(Activation('selu'))
model.add(Dropout(0.3))
model.add(Dense(100))
model.add(Dropout(0.3))
model.add(Activation('selu'))
model.add(Dense(50))
model.add(Activation('elu'))
model.add(Dense(1))
model.summary()

#Assigning and optimizating Learning rate
learning_rate=0.001
optimizer = opti.RMSprop(lr=learning_rate)
model.compile(optimizer=optimizer,loss='mse')

#Assigning data and target
data=X_train
target = y_train
model.fit(data, target, epochs=100, batch_size=10,verbose=0)

#Prediction
a=model.predict(X_test)
print("RMSE for NN:",np.sqrt(np.mean((y_test-a.reshape(a.size,))**2)))

#Plotting grid and scatterplot
plt.xlabel("Actual area burned")
plt.ylabel("Error")
plt.grid(True)
plt.scatter(10**(y_test),10**(a.reshape(a.size,))-10**(y_test))

plt.title("Histogram of prediction errors\n",fontsize=16)
plt.xlabel("Prediction error ($ha$)",fontsize=14)
plt.grid(True)
plt.hist(10**(a.reshape(a.size,))-10**(y_test),bins=50)

#Defining REC
rec_NN=[]
for i in range(tol_max):
    rec_NN.append(rec(a,y_test,i))
plt.figure(figsize=(5,5))
plt.title("REC curve for Neural Network\n",fontsize=16)
plt.xlabel("Absolute error (tolerance) in prediction ($ha$)")
plt.ylabel("Percentage of correct prediction")
plt.xticks([i for i in range(0,tol_max+1,5)])
plt.ylim(-10,100)
plt.yticks([i*20 for i in range(6)])
plt.grid(True)
plt.plot(range(tol_max),rec_NN)

# Relative performance of random forest and nn
plt.figure(figsize=(10,8))
plt.title("REC curve for RFR and NN\n",fontsize=20)
plt.xlabel("Absolute error (tolerance) in prediction ($ha$)",fontsize=15)
plt.ylabel("Percentage of correct prediction",fontsize=15)
plt.xticks([i for i in range(0,tol_max+1,1)],fontsize=13)
plt.ylim(-10,100)
plt.xlim(-2,tol_max)
plt.yticks([i*20 for i in range(6)],fontsize=18)
plt.grid(True)
plt.plot(range(tol_max),rec_RF,'--',lw=3)
plt.plot(range(tol_max),rec_NN,'-',lw=3)
plt.legend(['Random Forest','NN'],fontsize=13)

#Create object to linear regression
lm = LinearRegression()
lm.fit(X, y)

#Print intercept and coefficient
print(lm.intercept_)
print(lm.coef_)

(1.74449208e-02 * 54.29) + (-0.05086798058566372)  #y = mx+c
#m value--> coefficient(1.74449208e-02)
#c value--> intercept(-0.05086798058566372)

#Find predicted values through predict()
y_pred = lm.predict(X)
y_pred

#Compare actual value with predicted value
y==y_pred

# r2_value--> to check the model performance
from sklearn.metrics import r2_score
r2_score(y, y_pred)

#Print mean_squared_error as 'mse=' and mean_absolute_error as 'mae'
print("mse=", mse(y,y_pred))  #Printing mean_squared_error as 'mse='
print("mae=", mae(y,y_pred))  #Printing mean_absolute_error as 'mae='